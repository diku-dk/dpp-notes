\chapter{Cost Models}

When analysing the performance of algorithms, we do so not by
implementing them on some concrete machine and measuring their
performance, but rather by mathematically quantifying how many
``steps'' they need to finish. In your previous studies, you may have
used an intuitive notion of what a ``step'' is, such as counting
program statements or arithmetic operations. When analysing parallel
algorithms, we need more precision, since we are not just interested
in the total amount of work, but also the nature of the work---i.e.,
whether it is parallel.

\begin{definition}[Cost model]
  A mathematical model used for algorithmic analysis that defines the
  cost of various operations in some specified language. May contain
  one or more \emph{cost measures}.
\end{definition}

\begin{definition}[Cost measure]
  A (usually) discrete measure for the cost of a computation along
  some axis, such as sequential work, total work, or space usage.
\end{definition}

\begin{definition}[Work]
  A cost measure denoting the total amount of steps in an execution.
\end{definition}

\begin{definition}[Span/depth]
  A cost measure denoting the length of the longest sequential
  dependency chain in an execution.
\end{definition}

On an infinitely parallel computer, the span of a program is the
number of sequential steps needed to execute the program. Since
infinitely parallel computers are somewhat unlikely, we must wonder
whether the span makes sense on our tragically finite computer. The
answer is \emph{yes}: we can simulate an infinitely parallel computer
on a finitely parallel computer with overhead proportional to the
amount of ``missing'' parallelism. The theoretical justification for
this is \emph{Brent's Theorem}~\cite{10.1145/321812.321815}.

  \begin{theorem}[Brent's Theorem]
    Writing $T_{i}$ for the time taken to execute a program on $i$
    processors,
  \[
    \frac{T_{1}}{p} \leq T_{p} \leq T_{\infty} + \frac{T_{1}-T_{\infty}}{p}
  \]
  where $p$ is the number of available processors.

  % \begin{proof}
  %   At level $j$ of the computation DAG there are $M_{j}$ independent
  %   operations, which can clearly be executed by $p$ processors in
  %   time
  % \[
  %   \Bigl\lceil{\frac{M_{j}}{p}}\Bigr\rceil
  % \]
  % Now we have
  % \begin{align*}
  %   \frac{T_{1}}{p} &= \frac{\sum _{j} {M_{j}}}{p} & \text{(By definition)} \\
  %   & \leq \sum _{j} \Bigl\lceil{\frac{M_{j}}{p}}\Bigr\rceil \\
  %   & \leq \sum _{j} \Bigl\lceil{M_{j}}\Bigr\rceil
  % \end{align*}
  % \end{proof}

  \end{theorem}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "dpp-notes"
%%% End:
